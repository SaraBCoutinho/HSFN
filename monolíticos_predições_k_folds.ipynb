{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraBCoutinho/Pesquisa_Mestrado/blob/main/monol%C3%ADticos_predi%C3%A7%C3%B5es_k_folds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSKt1A_hfZLy"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rEB0MKIYfbfz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "from wordcloud import STOPWORDS, WordCloud\n",
        "!pip install ftfy\n",
        "import ftfy\n",
        "import nltk\n",
        "import json\n",
        "import re\n",
        "%matplotlib inline\n",
        "import csv\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(0)\n",
        "\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV,cross_validate, StratifiedKFold\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer, StandardScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import tree\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from os import listdir, makedirs\n",
        "from os.path import isfile, join, splitext, split\n",
        "from unicodedata import normalize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "stopwords_list = stopwords.words('english')\n",
        "import re\n",
        "import string\n",
        "!pip install zeugma\n",
        "!pip install preprocessing\n",
        "!pip install function\n",
        "from zeugma.embeddings import EmbeddingTransformer\n",
        "from preprocessing import *\n",
        "from function import *\n",
        "import itertools\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFUxgNvQlxXk"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_kaggle = pd.read_csv('/content/kaggle.csv')\n",
        "data_kaggle.rename(columns = {'Body':'statement','Label':'label'}, inplace = True)\n",
        "data_kaggle = data_kaggle.copy().dropna()\n",
        "data_kaggle = data_kaggle[['statement','label']]\n",
        "print(\"frequency of kaggle labels: \", data_kaggle['label'].value_counts())\n",
        "print(\"null values kaggle: \", pd.isnull(data_kaggle['statement']).sum(), pd.isnull(data_kaggle['label']).sum(), pd.isnull(data_kaggle).sum())\n",
        "#data.info()\n",
        "data_kaggle.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "prtSP6uYF5P2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fc55e9-903b-43d0-e907-bf4d25e29be8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frequency of kaggle labels:  0    2120\n",
            "1    1868\n",
            "Name: label, dtype: int64\n",
            "null values kaggle:  0 0 statement    0\n",
            "label        0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZtWWlWa2h6A"
      },
      "source": [
        "### Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "stopwords_list = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
        "    text = text.translate(translate_table)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    re_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",re.MULTILINE | re.UNICODE)\n",
        "    re_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n",
        "    text = re_url.sub(\"URL\", text)\n",
        "    text = re_ip.sub(\"IPADDRESS\", text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "#    text = \" \".join(stemmed_words)\n",
        "#    text = text.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in stemmed_words if not w in stops and len(w) >= 3]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "def tokenizer(text, flg_stemm=True, flg_lemm=False, lst_stopwords=stopwords_list):\n",
        "    lst_text = text.lower().split()\n",
        "\n",
        "    if flg_stemm == True:\n",
        "        ps = nltk.stem.porter.PorterStemmer()\n",
        "        lst_text = [ps.stem(word) for word in lst_text]\n",
        "\n",
        "    if flg_lemm == True:\n",
        "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
        "\n",
        "    text = \" \".join(lst_text)\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5Wkg2ovIMnR",
        "outputId": "3c224113-91fd-4ae3-f1e9-111b1c8ec57e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 450 µs, sys: 0 ns, total: 450 µs\n",
            "Wall time: 460 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_kaggle['statement']=data_kaggle['statement'].map(clean_text)\n",
        "data_kaggle_to_count=data_kaggle.copy()\n",
        "\n",
        "def CountOnly(sentence):\n",
        "  sentence=sentence.split()\n",
        "  result = Counter(sentence)\n",
        "  return result\n",
        "\n",
        "data_kaggle_to_count['statement']=data_kaggle_to_count['statement'].apply(CountOnly)\n",
        "new_data_kaggle=Counter()\n",
        "\n",
        "for sentence in data_kaggle_to_count['statement']:\n",
        "  new_data_kaggle = new_data_kaggle + sentence\n",
        "\n",
        "list_unique_data_kaggle=[]\n",
        "for k,v in new_data_kaggle.items():\n",
        "  if v==1:\n",
        "    list_unique_data_kaggle.append(k)\n",
        "\n",
        "total_data_kaggle=0\n",
        "for k,v in new_data_kaggle.items():\n",
        "  total_data_kaggle = total_data_kaggle + v\n",
        "\n",
        "print(total_data_kaggle)\n",
        "print(len(list_unique_data_kaggle))\n",
        "print(\"0 conjunto de dados atual final correspondeu a aproximadamente: \",\n",
        "      round(((total_data_kaggle-len(list_unique_data_kaggle))/total_data_kaggle)*100,1), \"% do conjunto original com um total de: \",(total_data_kaggle-len(list_unique_data_kaggle)),\"palavras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBhrA_dzvy9j",
        "outputId": "3c483d03-56a8-4806-8d58-686ddf8670c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1102335\n",
            "12994\n",
            "0 conjunto de dados atual final correspondeu a aproximadamente:  98.8 % do conjunto original com um total de:  1089341 palavras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def cleanningOnceDataKaggle(statement):\n",
        "  statement=statement.split()\n",
        "  result = [word for word in statement if word not in list_unique_data_kaggle]\n",
        "  result = \" \".join(result)\n",
        "  return result\n",
        "\n",
        "data_kaggle['statement']=data_kaggle['statement'].apply(cleanningOnceDataKaggle)"
      ],
      "metadata": {
        "id": "7-sA0BgAumb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73592e7f-4542-46ca-dd05-a08179a5ad9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6min 19s, sys: 1.04 s, total: 6min 20s\n",
            "Wall time: 6min 26s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#0 fake 1 true\n",
        "print(data_kaggle['label'].count())\n",
        "print(data_kaggle['label'].value_counts())\n",
        "print(data_kaggle['label'].value_counts(1))\n",
        "data_kaggle=data_kaggle.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV4uuuY1zncY",
        "outputId": "6dfe2c53-0fd2-4f4b-a58d-84e3c340a69f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3988\n",
            "0    2120\n",
            "1    1868\n",
            "Name: label, dtype: int64\n",
            "0    0.531595\n",
            "1    0.468405\n",
            "Name: label, dtype: float64\n",
            "CPU times: user 6.29 ms, sys: 0 ns, total: 6.29 ms\n",
            "Wall time: 6.31 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf '/content/Data/'\n",
        "!mkdir Data\n",
        "!mkdir Data/kaggle\n",
        "!mkdir Data/kaggle/F1\n",
        "!mkdir Data/kaggle/F2\n",
        "!mkdir Data/kaggle/F3\n",
        "!mkdir Data/kaggle/F4\n",
        "!mkdir Data/kaggle/F5\n",
        "!mkdir Data/kaggle/F6\n",
        "!mkdir Data/kaggle/F7\n",
        "!mkdir Data/kaggle/F8\n",
        "!mkdir Data/kaggle/F9\n",
        "!mkdir Data/kaggle/F10"
      ],
      "metadata": {
        "id": "7kRpunSj0MDn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=10)\n",
        "\n",
        "for db in ['kaggle']:\n",
        "    if db == 'kaggle':\n",
        "      X, y = data_kaggle['statement'], data_kaggle['label']\n",
        "      folds = kfold.split(X, y)\n",
        "      dataset_name = 'kaggle'\n",
        "      path = '/content/Data/'+dataset_name+'/'\n",
        "      for fold, data in enumerate(folds):\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "          X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n",
        "\n",
        "          folder = path + 'F' + str(fold+1) + '/'\n",
        "          X_train.to_csv(folder+'train.csv')\n",
        "          X_test.to_csv(folder+'test.csv')\n",
        "          X_val.to_csv(folder+'val.csv')\n",
        "\n",
        "          y_train.to_csv(folder+'train_labels.csv')\n",
        "          y_test.to_csv(folder+'test_labels.csv')\n",
        "          y_val.to_csv(folder+'val_labels.csv')\n",
        "\n",
        "          train = pd.concat([X_train,y_train], axis= 1)\n",
        "          train.to_csv(folder+'train_full.csv')\n",
        "          val = pd.concat([X_val,y_val], axis= 1)\n",
        "          val.to_csv(folder+'val_full.csv')\n",
        "          test = pd.concat([X_test,y_test], axis= 1)\n",
        "          test.to_csv(folder+'test_full.csv')"
      ],
      "metadata": {
        "id": "v_17TVS0rAva"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vm0CKXuPOAF"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOcU6b4sXZUS",
        "outputId": "c8b58539-4ec2-423d-f9c5-593fd8394a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "w2v = EmbeddingTransformer('word2vec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IWV0OtTuJoX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d349722-9dfc-473e-ef29-0ce36a54de74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "glove = EmbeddingTransformer('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gT9zOABnXh-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719a8d84-6da3-4350-9fae-b8222a6fb881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "fasttext = EmbeddingTransformer('fasttext')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7Ggpf1s-XkB5"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(analyzer='word', lowercase=True, stop_words='english')\n",
        "tfidf =  TfidfVectorizer(analyzer='word', lowercase=True, use_idf=True, stop_words='english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTVBnwFiHB0b"
      },
      "source": [
        "## PARTE II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZguzMJwHYad"
      },
      "source": [
        "### Classificadores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funções"
      ],
      "metadata": {
        "id": "cVWVHWp7TAyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zhGWxdAe95S8"
      },
      "outputs": [],
      "source": [
        "#--Classifiers\n",
        "#--SVM\n",
        "def get_svm(cv, tfidf, w2v, glove, fasttext):\n",
        "    return{\n",
        "        'CV': {'CLF': SVC(random_state=42, kernel='linear', gamma=0.1, probability=True),'EXT': cv,},\n",
        "        'TFIDF': {'CLF': SVC(random_state=42, kernel='linear',\n",
        "                             gamma=0.1, probability=True),'EXT': tfidf,},\n",
        "        'W2V': {'CLF': SVC(random_state=42, kernel='rbf', gamma=1, probability=True),'EXT': w2v,},\n",
        "        'GLOVE': {'CLF': SVC(random_state=42, kernel='rbf', gamma=0.5, probability=True),'EXT':  glove,},\n",
        "        'FAST': {'CLF': SVC(random_state=42, kernel='rbf', gamma=1, probability=True),'EXT': fasttext,}\n",
        "    }\n",
        "#--LR\n",
        "def get_lr(cv,tfidf,glove,w2v,fasttext):\n",
        "    return {\n",
        "        'CV': {'CLF': LogisticRegression(random_state=42,multi_class='auto', solver='liblinear', penalty='l1'),'EXT': cv,},\n",
        "        'TFIDF': {'CLF': LogisticRegression(random_state=42, multi_class='auto', solver='liblinear', penalty='l1'),'EXT': tfidf,},\n",
        "        'GLOVE': {'CLF': LogisticRegression(random_state=42, multi_class='auto', solver='liblinear', penalty='l1'),'EXT':  glove,},\n",
        "        'W2V': {'CLF': LogisticRegression(random_state=42, multi_class='auto', solver='liblinear', penalty='l1'),'EXT': w2v,},\n",
        "        'FAST': {'CLF': LogisticRegression(random_state=42, multi_class='auto', solver='liblinear', penalty='l1'),'EXT': fasttext,}\n",
        "        }\n",
        "#--RF\n",
        "def get_rf(cv, tfidf, w2v, glove, fasttext):\n",
        "    return {'CV': {'CLF': RandomForestClassifier(random_state=42, verbose=100, n_estimators=20, n_jobs=-1),'EXT': cv,},\n",
        "        'TFIDF': {'CLF': RandomForestClassifier(random_state=42, verbose=100, n_estimators=50, n_jobs=-1),'EXT': tfidf,},\n",
        "        'W2V': {'CLF': RandomForestClassifier(random_state=42, verbose=100, n_estimators=50, n_jobs=-1),'EXT': w2v,},\n",
        "        'GLOVE': {'CLF': RandomForestClassifier(random_state=42, verbose=100, n_estimators=50, n_jobs=-1),'EXT':  glove,},\n",
        "        'FAST': {'CLF': RandomForestClassifier(random_state=42, verbose=100, n_estimators=50, n_jobs=-1),'EXT': fasttext,}\n",
        "            }\n",
        "#--NB\n",
        "def get_nb(cv, tfidf, w2v, glove, fasttext):\n",
        "    return {\n",
        "        'CV': {'CLF': MultinomialNB(alpha=1, fit_prior=False),'EXT': cv,},\n",
        "        'TFIDF': {'CLF': MultinomialNB(alpha=0.5, fit_prior=False),'EXT': tfidf,},\n",
        "        'W2V': {'CLF': BernoulliNB(alpha=0.5, fit_prior=True),'EXT': w2v,},\n",
        "        'GLOVE': {'CLF': BernoulliNB(alpha=0.1, fit_prior=True),'EXT':  glove,},\n",
        "        'FAST': {'CLF': BernoulliNB(alpha=1, fit_prior=True),'EXT': fasttext,}\n",
        "    }\n",
        "#--MLP\n",
        "def get_mlp(cv, tfidf, w2v, glove, fasttext):\n",
        "    return {'CV': {'CLF': MLPClassifier(random_state=42, batch_size=20, max_iter=40,activation='relu', solver='lbfgs'),'EXT': cv,},\n",
        "        'TFIDF': {'CLF': MLPClassifier(random_state=42, batch_size=20, max_iter=40,activation='logistic', solver='adam'),'EXT': tfidf,},\n",
        "        'W2V': {'CLF': MLPClassifier(random_state=42, batch_size=20, max_iter=40, activation='relu', solver='adam'),'EXT': w2v,},\n",
        "        'GLOVE': {'CLF': MLPClassifier(random_state=42, batch_size=20, max_iter=40, activation='relu', solver='adam'),'EXT':  glove,},\n",
        "        'FAST': {'CLF': MLPClassifier(random_state=42, batch_size=20, max_iter=40, activation='relu', solver='adam'),'EXT': fasttext,}\n",
        "    }\n",
        "#--EXTRA\n",
        "def get_extra(cv, tfidf, w2v, glove, fasttext):\n",
        "    return {\n",
        "        'CV': {'CLF': ExtraTreesClassifier(random_state=42, n_estimators=50, n_jobs=-1),'EXT': cv,},\n",
        "        'TFIDF': {'CLF': ExtraTreesClassifier(random_state=42, n_estimators=50, n_jobs=-1),'EXT': tfidf,},\n",
        "        'W2V': {'CLF': ExtraTreesClassifier(random_state=42, n_estimators=50, n_jobs=-1),'EXT': w2v,},\n",
        "        'GLOVE': {'CLF': ExtraTreesClassifier(random_state=42, n_estimators=50, n_jobs=-1),'EXT':  glove,},\n",
        "        'FAST': {'CLF': ExtraTreesClassifier(random_state=42, n_estimators=50, n_jobs=-1),'EXT': fasttext,}\n",
        "    }\n",
        "#--KNN\n",
        "def get_knn(cv, tfidf, w2v, glove, fasttext):\n",
        "    return {\n",
        "        'CV': {'CLF': KNeighborsClassifier(n_neighbors=3, n_jobs=-1),'EXT': cv,},\n",
        "        'TFIDF': {'CLF': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),'EXT': tfidf,},\n",
        "        'W2V': {'CLF': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),'EXT': w2v,},\n",
        "        'GLOVE': {'CLF': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),'EXT':  glove,},\n",
        "        'FAST': {'CLF': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),'EXT': fasttext,}\n",
        "    }\n",
        "\n",
        "def get_CNN_2(ext, tokenizer, MAX_NB_WORDS, EMBEDDING_DIM=300, MAX_SEQUENCE_LENGTH=300, activation='sigmoid', word_embedding=False, dense=2):\n",
        "    if word_embedding == False:\n",
        "        X_ext = ext.get_feature_names()\n",
        "        model = Word2Vec([X_ext], min_count=1, workers=1, size=300)\n",
        "    else:\n",
        "      model = ext.model\n",
        "\n",
        "    tokenizer = tokenizer\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    nb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
        "\n",
        "    embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if word in model.wv.vocab:\n",
        "            embedding_matrix[i] = model.wv.word_vec(word)\n",
        "\n",
        "\n",
        "    embedding_layer = Embedding(nb_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False\n",
        "                                )\n",
        "    cnn = Sequential()\n",
        "    cnn.add(embedding_layer)\n",
        "    cnn.add(Dropout(0.2))\n",
        "    cnn.add(Conv1D(64, 2, padding='valid', activation='relu', strides=1))\n",
        "    cnn.add(GlobalMaxPooling1D())\n",
        "    cnn.add(Dense(256))\n",
        "    cnn.add(Dropout(0.2))\n",
        "    cnn.add(Activation('relu'))\n",
        "    cnn.add(Dense(dense))\n",
        "    cnn.add(Activation(activation))\n",
        "#    cnn.add(Reshape(None,2))\n",
        "    cnn.summary()\n",
        "    cnn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    return cnn\n",
        "\n",
        "def get_classifier(clf, X, y, ext):\n",
        "    pipe_clf = Pipeline([('extractor', ext),('clf', clf)])\n",
        "    pipe_clf.fit(X, y)\n",
        "    return pipe_clf\n",
        "\n",
        "def update_pred_proba_2(clfs, label, train, val, test):\n",
        "    pred_train = pd.DataFrame()\n",
        "    pred_val = pd.DataFrame()\n",
        "    pred_test = pd.DataFrame()\n",
        "\n",
        "    prob_train = pd.DataFrame()\n",
        "    prob_val = pd.DataFrame()\n",
        "    prob_test = pd.DataFrame()\n",
        "\n",
        "    for ext, clf in clfs.items():\n",
        "        # Predict\n",
        "        df_pred_train_ = pd.DataFrame(clf.predict(train), columns=[\"{}-{}\".format(label, ext)])\n",
        "        df_pred_val_ = pd.DataFrame(clf.predict(val), columns=[\"{}-{}\".format(label, ext)])\n",
        "        df_pred_test_ = pd.DataFrame(clf.predict(test), columns=[\"{}-{}\".format(label, ext)])\n",
        "\n",
        "        # # Probabilidades\n",
        "        cols = [\n",
        "            \"{}-{}-{}\".format(label, ext, clf.classes_[0]),\n",
        "            \"{}-{}-{}\".format(label, ext, clf.classes_[1])\n",
        "        ]\n",
        "        df_prob_train_ = pd.DataFrame(clf.predict_proba(train), columns=cols)\n",
        "        df_prob_val_ = pd.DataFrame(clf.predict_proba(val), columns=cols)\n",
        "        df_prob_test_ = pd.DataFrame(clf.predict_proba(test), columns=cols)\n",
        "\n",
        "        pred_train = pd.concat([pred_train, df_pred_train_], axis=1, sort=False)\n",
        "        pred_val = pd.concat([pred_val, df_pred_val_], axis=1, sort=False)\n",
        "        pred_test = pd.concat([pred_test, df_pred_test_], axis=1, sort=False)\n",
        "\n",
        "        prob_train = pd.concat([prob_train, df_prob_train_], axis=1, sort=False)\n",
        "        prob_val = pd.concat([prob_val, df_prob_val_], axis=1, sort=False)\n",
        "        prob_test = pd.concat([prob_test, df_prob_test_], axis=1, sort=False)\n",
        "\n",
        "    return  pred_train, pred_val, pred_test, prob_train, prob_val, prob_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Gerff2dKYq"
      },
      "source": [
        "#### Classificação  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=25000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "print(len(tokenizer.word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Amie12qquL",
        "outputId": "3b788bc7-29e3-4fca-b55d-f2063375ddd7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def main():\n",
        "  for db in [\"kaggle\"]:\n",
        "    if db == 'kaggle':\n",
        "      classe = 'label'\n",
        "      for fold in [\"F1\", \"F2\", \"F3\", \"F4\", \"F5\",\"F6\", \"F7\", \"F8\", \"F9\", \"F10\"]:\n",
        "\n",
        "        df_pred_train = pd.DataFrame()\n",
        "        df_pred_val = pd.DataFrame()\n",
        "        df_pred_test = pd.DataFrame()\n",
        "\n",
        "        df_prob_train = pd.DataFrame()\n",
        "        df_prob_val = pd.DataFrame()\n",
        "        df_prob_test = pd.DataFrame()\n",
        "\n",
        "        X_train = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/train_full.csv')['statement']\n",
        "        y_train = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/train_full.csv')['label']\n",
        "        train = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/train_full.csv')\n",
        "\n",
        "        X_test = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/test_full.csv')['statement']\n",
        "        y_test = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/test_full.csv')['label']\n",
        "        test = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/test_full.csv')\n",
        "\n",
        "        X_val = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/val_full.csv')['statement']\n",
        "        y_val = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/val_full.csv')['label']\n",
        "        val = pd.read_csv('/content/Data/' + db + \"/\" + fold+'/val_full.csv')\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, train['label']], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, val['label']], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, test['label']], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, train['label']], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, val['label']], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, test['label']], axis=1, sort=False)\n",
        "\n",
        "        print('dados ok')\n",
        "\n",
        "        # SVM\n",
        "        classifier = get_svm(cv, tfidf, w2v, glove, fasttext)\n",
        "        clfs = { }\n",
        "\n",
        "        for ext, clf in classifier.items():\n",
        "          clfs[ext] = get_classifier(clf['CLF'], X_train, y_train, clf['EXT'])\n",
        "        pred_train, pred_val, pred_test, prob_train, prob_val, prob_test = update_pred_proba_2(clfs, \"SVM\", X_train, X_val, X_test)\n",
        "        print('svm parte 1 ok')\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, pred_train], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, pred_val], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, pred_test], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, prob_train], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, prob_val], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, prob_test], axis=1, sort=False)\n",
        "\n",
        "        print('svm parte 2 ok')\n",
        "\n",
        "        # LR\n",
        "        classifier = get_lr(cv,tfidf,glove,w2v,fasttext)\n",
        "        clfs = { }\n",
        "\n",
        "        for ext, clf in classifier.items():\n",
        "          clfs[ext] = get_classifier(clf['CLF'], X_train, y_train, clf['EXT'])\n",
        "\n",
        "        pred_train, pred_val, pred_test, prob_train, prob_val, prob_test = update_pred_proba_2(clfs, \"LR\", X_train, X_val, X_test)\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, pred_train], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, pred_val], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, pred_test], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, prob_train], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, prob_val], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, prob_test], axis=1, sort=False)\n",
        "\n",
        "        print('lr ok')\n",
        "\n",
        "        # RF\n",
        "        classifier = get_rf(cv, tfidf, w2v, glove, fasttext)\n",
        "        # clfs = { }\n",
        "        for ext, clf in classifier.items():\n",
        "            clfs.update({ext: get_classifier(clf['CLF'], X_train, y_train, clf['EXT'])})\n",
        "\n",
        "        pred_train, pred_val, pred_test, prob_train, prob_val, prob_test = update_pred_proba_2(clfs, \"RF\", X_train, X_val, X_test)\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, pred_train], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, pred_val], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, pred_test], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, prob_train], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, prob_val], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, prob_test], axis=1, sort=False)\n",
        "\n",
        "        print('rf ok')\n",
        "\n",
        "        # NB\n",
        "        classifier = get_nb(cv, tfidf, w2v, glove, fasttext)\n",
        "        # clfs = { }\n",
        "        for ext, clf in classifier.items():\n",
        "            clfs.update({ext: get_classifier(clf['CLF'], X_train, y_train, clf['EXT'])})\n",
        "\n",
        "        pred_train, pred_val, pred_test, prob_train, prob_val, prob_test = update_pred_proba_2(clfs, \"NB\", X_train, X_val, X_test)\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, pred_train], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, pred_val], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, pred_test], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, prob_train], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, prob_val], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, prob_test], axis=1, sort=False)\n",
        "\n",
        "        print('nb ok')\n",
        "\n",
        "        # MLP\n",
        "        classifier = get_mlp(cv, tfidf, w2v, glove, fasttext)\n",
        "        # clfs = { }\n",
        "        for ext, clf in classifier.items():\n",
        "            clfs.update({ext: get_classifier(clf['CLF'], X_train, y_train, clf['EXT'])})\n",
        "\n",
        "        pred_train, pred_val, pred_test, prob_train, prob_val, prob_test = update_pred_proba_2(clfs, \"MLP\", X_train, X_val, X_test)\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, pred_train], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, pred_val], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, pred_test], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, prob_train], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, prob_val], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, prob_test], axis=1, sort=False)\n",
        "\n",
        "        print('mlp ok')\n",
        "\n",
        "        # Extra\n",
        "        classifier = get_extra(cv, tfidf, w2v, glove, fasttext)\n",
        "        # clfs = { }\n",
        "        for ext, clf in classifier.items():\n",
        "            clfs.update({ext: get_classifier(clf['CLF'], X_train, y_train, clf['EXT'])})\n",
        "\n",
        "        pred_train, pred_val, pred_test, prob_train, prob_val, prob_test = update_pred_proba_2(clfs, \"EXTRA\", X_train, X_val, X_test)\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, pred_train], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, pred_val], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, pred_test], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, prob_train], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, prob_val], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, prob_test], axis=1, sort=False)\n",
        "\n",
        "        print('extra ok')\n",
        "\n",
        "        # KNN\n",
        "        classifier = get_knn(cv, tfidf, w2v, glove, fasttext)\n",
        "        # clfs = { }\n",
        "        for ext, clf in classifier.items():\n",
        "            clfs.update({ext: get_classifier(clf['CLF'], X_train, y_train, clf['EXT'])})\n",
        "\n",
        "        pred_train, pred_val, pred_test, prob_train, prob_val, prob_test = update_pred_proba_2(clfs, \"KNN\", X_train, X_val, X_test)\n",
        "\n",
        "\n",
        "        df_pred_train = pd.concat([df_pred_train, pred_train], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, pred_val], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, pred_test], axis=1, sort=False)\n",
        "\n",
        "        df_prob_train = pd.concat([df_prob_train, prob_train], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, prob_val], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, prob_test], axis=1, sort=False)\n",
        "\n",
        "        print('knn ok')\n",
        "\n",
        "        #CNN\n",
        "        MAX_NB_WORDS = 25000\n",
        "        MAX_SEQUENCE_LENGTH=300\n",
        "\n",
        "        train_labels = train['label']\n",
        "        val_labels = val['label']\n",
        "        test_labels = test['label']\n",
        "\n",
        "        y_train = to_categorical(train_labels)\n",
        "        y_val = to_categorical(val_labels)\n",
        "        y_test = to_categorical(test_labels)\n",
        "\n",
        "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "        tokenizer.fit_on_texts(X_train) #rever por que\n",
        "        seq_train = tokenizer.texts_to_sequences(X_train)\n",
        "        seq_val = tokenizer.texts_to_sequences(X_val)\n",
        "        seq_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "        data_train = pad_sequences(seq_train,padding='pre', truncating='pre',maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        data_val = pad_sequences(seq_val,padding='pre', truncating='pre',maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        data_test = pad_sequences(seq_test, padding='pre', truncating='pre',maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "        y_validation=to_categorical(y_val, num_classes=2)\n",
        "        y_training=to_categorical(y_train, num_classes=2)\n",
        "        y_testing=to_categorical(y_test, num_classes=2)\n",
        "\n",
        "        # NET\n",
        "        cnn_cv = get_CNN_2(cv, tokenizer, MAX_NB_WORDS, EMBEDDING_DIM=300, activation='sigmoid', dense=2)\n",
        "        cnn_tfidf = get_CNN_2(tfidf, tokenizer, MAX_NB_WORDS, EMBEDDING_DIM=300, activation='sigmoid', dense=2)\n",
        "        cnn_w2v = get_CNN_2(w2v, tokenizer, MAX_NB_WORDS, EMBEDDING_DIM=300, activation='sigmoid', word_embedding=True, dense=2)\n",
        "        cnn_glove = get_CNN_2(glove, tokenizer, MAX_NB_WORDS, EMBEDDING_DIM=25, activation='sigmoid', word_embedding=True, dense=2)\n",
        "        cnn_fast = get_CNN_2(fasttext, tokenizer, MAX_NB_WORDS, EMBEDDING_DIM=300, activation='sigmoid', word_embedding=True, dense=2)\n",
        "\n",
        "        # TRAIN\n",
        "        #cnn_cv.fit(data_train, y_training, validation_data=(data_val,y_validation), epochs=20, batch_size=20)\n",
        "        cnn_cv.fit(data_train, y_train, validation_data=(data_val,y_val), epochs=20, batch_size=20)\n",
        "        cnn_tfidf.fit(data_train, y_train, validation_data=(data_val,y_val), epochs=20, batch_size=20)\n",
        "        cnn_w2v.fit(data_train, y_train, validation_data=(data_val, y_val), epochs=20, batch_size=200)\n",
        "        cnn_glove.fit(data_train, y_train, validation_data=(data_val, y_val), epochs=20, batch_size=200)\n",
        "        cnn_fast.fit(data_train, y_train, validation_data=(data_val, y_val), epochs=20, batch_size=200)\n",
        "\n",
        "        cols = [\"CNN-CV-0\", \"CNN-CV-1\"]\n",
        "\n",
        "        df_train_ = pd.DataFrame(np.argmax(cnn_cv.predict(data_train), axis=1), columns=[\"CNN-CV\"])\n",
        "        df_val_ = pd.DataFrame(np.argmax(cnn_cv.predict(data_val), axis=1), columns=[\"CNN-CV\"])\n",
        "        df_test_ = pd.DataFrame(np.argmax(cnn_cv.predict(data_test), axis=1), columns=[\"CNN-CV\"])\n",
        "        df_pred_train = pd.concat([df_pred_train, df_train_], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, df_val_], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, df_test_], axis=1, sort=False)\n",
        "        # Probabilidades\n",
        "        df_train_ = pd.DataFrame(cnn_cv.predict(data_train), columns=cols)\n",
        "        df_val_ = pd.DataFrame(cnn_cv.predict(data_val), columns=cols)\n",
        "        df_test_ = pd.DataFrame(cnn_cv.predict(data_test), columns=cols)\n",
        "        df_prob_train = pd.concat([df_prob_train, df_train_], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, df_val_], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, df_test_], axis=1, sort=False)\n",
        "\n",
        "        cols = [\"CNN-TF-0\", \"CNN-TF-1\"]\n",
        "        df_train_ = pd.DataFrame(np.argmax(cnn_tfidf.predict(data_train), axis=1), columns=[\"CNN-TFIDF\"])\n",
        "        df_val_ = pd.DataFrame(np.argmax(cnn_tfidf.predict(data_val), axis=1), columns=[\"CNN-TFIDF\"])\n",
        "        df_test_ = pd.DataFrame(np.argmax(cnn_tfidf.predict(data_test), axis=1), columns=[\"CNN-TFIDF\"])\n",
        "        df_pred_train = pd.concat([df_pred_train, df_train_], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, df_val_], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, df_test_], axis=1, sort=False)\n",
        "        # Probabilidades\n",
        "        df_train_ = pd.DataFrame(cnn_tfidf.predict(data_train), columns=cols)\n",
        "        df_val_ = pd.DataFrame(cnn_tfidf.predict(data_val), columns=cols)\n",
        "        df_test_ = pd.DataFrame(cnn_tfidf.predict(data_test), columns=cols)\n",
        "        df_prob_train = pd.concat([df_prob_train, df_train_], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, df_val_], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, df_test_], axis=1, sort=False)\n",
        "\n",
        "        cols = [\"CNN-W2V-0\", \"CNN-W2V-1\"]\n",
        "        df_train_ = pd.DataFrame(np.argmax(cnn_w2v.predict(data_train), axis=1), columns=[\"CNN-W2V\"])\n",
        "        df_val_ = pd.DataFrame(np.argmax(cnn_w2v.predict(data_val), axis=1), columns=[\"CNN-W2V\"])\n",
        "        df_test_ = pd.DataFrame(np.argmax(cnn_w2v.predict(data_test), axis=1), columns=[\"CNN-W2V\"])\n",
        "        df_pred_train = pd.concat([df_pred_train, df_train_], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, df_val_], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, df_test_], axis=1, sort=False)\n",
        "        # Probabilidades\n",
        "        df_train_ = pd.DataFrame(cnn_w2v.predict(data_train), columns=cols)\n",
        "        df_val_ = pd.DataFrame(cnn_w2v.predict(data_val), columns=cols)\n",
        "        df_test_ = pd.DataFrame(cnn_w2v.predict(data_test), columns=cols)\n",
        "        df_prob_train = pd.concat([df_prob_train, df_train_], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, df_val_], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, df_test_], axis=1, sort=False)\n",
        "\n",
        "        cols = [\"CNN-GLOVE-0\", \"CNN-GLOVE-1\"]\n",
        "        df_train_ = pd.DataFrame(np.argmax(cnn_glove.predict(data_train), axis=1), columns=[\"CNN-GLOVE\"])\n",
        "        df_val_ = pd.DataFrame(np.argmax(cnn_glove.predict(data_val), axis=1), columns=[\"CNN-GLOVE\"])\n",
        "        df_test_ = pd.DataFrame(np.argmax(cnn_glove.predict(data_test), axis=1), columns=[\"CNN-GLOVE\"])\n",
        "        df_pred_train = pd.concat([df_pred_train, df_train_], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, df_val_], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, df_test_], axis=1, sort=False)\n",
        "        # Probabilidades\n",
        "        df_train_ = pd.DataFrame(cnn_glove.predict(data_train), columns=cols)\n",
        "        df_val_ = pd.DataFrame(cnn_glove.predict(data_val), columns=cols)\n",
        "        df_test_ = pd.DataFrame(cnn_glove.predict(data_test), columns=cols)\n",
        "        df_prob_train = pd.concat([df_prob_train, df_train_], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, df_val_], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, df_test_], axis=1, sort=False)\n",
        "\n",
        "        cols = [\"CNN-FAST-0\", \"CNN-FAST-1\"]\n",
        "        df_train_ = pd.DataFrame(np.argmax(cnn_fast.predict(data_train), axis=1), columns=[\"CNN-FAST\"])\n",
        "        df_val_ = pd.DataFrame(np.argmax(cnn_fast.predict(data_val), axis=1), columns=[\"CNN-FAST\"])\n",
        "        df_test_ = pd.DataFrame(np.argmax(cnn_fast.predict(data_test), axis=1), columns=[\"CNN-FAST\"])\n",
        "        df_pred_train = pd.concat([df_pred_train, df_train_], axis=1, sort=False)\n",
        "        df_pred_val = pd.concat([df_pred_val, df_val_], axis=1, sort=False)\n",
        "        df_pred_test = pd.concat([df_pred_test, df_test_], axis=1, sort=False)\n",
        "        # Probabilidades\n",
        "        df_train_ = pd.DataFrame(cnn_fast.predict(data_train), columns=cols)\n",
        "        df_val_ = pd.DataFrame(cnn_fast.predict(data_val), columns=cols)\n",
        "        df_test_ = pd.DataFrame(cnn_fast.predict(data_test), columns=cols)\n",
        "        df_prob_train = pd.concat([df_prob_train, df_train_], axis=1, sort=False)\n",
        "        df_prob_val = pd.concat([df_prob_val, df_val_], axis=1, sort=False)\n",
        "        df_prob_test = pd.concat([df_prob_test, df_test_], axis=1, sort=False)\n",
        "\n",
        "        print('cnn ok')\n",
        "\n",
        "        # Probabilidade\n",
        "        path = \"./Data/{}/{}\".format(db,fold)\n",
        "        df_pred_train.to_csv(\"{}/pred_train.csv\".format(path))\n",
        "        df_pred_val.to_csv(\"{}/pred_val.csv\".format(path))\n",
        "        df_pred_test.to_csv(\"{}/pred_test.csv\".format(path))\n",
        "\n",
        "        df_prob_train.to_csv(\"{}/prob_train.csv\".format(path))\n",
        "        df_prob_val.to_csv(\"{}/prob_val.csv\".format(path))\n",
        "        df_prob_test.to_csv(\"{}/prob_test.csv\".format(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKaR_EgXHu5Z",
        "outputId": "65314797-adaa-4e98-d480-d212a8d00b32"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
            "Wall time: 11.7 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fo9QWAUdDs-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}